{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Text Classification with Pre-Trained Word Embeddings\n",
    "This notebook provides code to reproduce the results from our paper, Few-Shot Text Classification with Pre-Trained Word Embeddings and a Human in the Loop. Specifically, the results obtained on the 20 Newsgroups dataset.\n",
    "\n",
    "To begin, some setup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from operator import itemgetter\n",
    "from itertools import cycle, islice\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sif_embedding_wrapper\n",
    "import utils\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pre-trained GloVe embeddings\n",
    "Here we use the SIF code to load up the words, embeddings and weights we'll be using to vectorize our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, embs, weight4ind = sif_embedding_wrapper.load_embeddings(\"/tmp/glove.6B/glove.6B.300d.txt\", \n",
    "                                                     '/tmp/enwiki_vocab_min200.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification example with 20 Newsgroups\n",
    "Here we define a batch of documents to be classified, based on a subset of categories from the 20 Newsgroups dataset. It extracts two dataframes, one that holds the document ID and corresponding text for each document, and another that holds the document ID and corresponding ground truth category for each document. It returns these dataframes, along with a list of the simplified category names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_for_newsgroup_pair(category_pair):\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train', categories=category_pair, remove=('headers', 'footers', 'quotes'))\n",
    "    docs = {}\n",
    "    for i,text in enumerate(newsgroups_train.data):\n",
    "        doc_id = str(i+1)\n",
    "        docs[doc_id] = {\n",
    "            \"text\": text.strip().strip('\"'),\n",
    "            \"category_ind\": newsgroups_train.target[i]\n",
    "        }\n",
    "    all_doc_ids = sorted(list(docs.keys()))\n",
    "    df = pd.DataFrame({\"text\": [docs[d][\"text\"] for d in all_doc_ids], \n",
    "                       \"category_ind\": [docs[d][\"category_ind\"] for d in all_doc_ids], \n",
    "                       \"doc_id\": [d for d in all_doc_ids]})\n",
    "    labels = []\n",
    "    for i in df[\"category_ind\"]:\n",
    "        parts = newsgroups_train.target_names[i].split(\".\")\n",
    "        if parts[-1] == \"misc\":\n",
    "            labels.append(parts[-2])\n",
    "        else:\n",
    "            labels.append(parts[-1])\n",
    "    df[\"label\"] = labels\n",
    "    categories = list(df[\"label\"].unique())\n",
    "    text_df = pd.DataFrame({\"doc_id\": df[\"doc_id\"], \"text\": df[\"text\"]})\n",
    "    truth_df = pd.DataFrame({\"doc_id\": df[\"doc_id\"], \"gt\": df[\"label\"]})\n",
    "    truth_dict = {str(rec[\"doc_id\"]): rec[\"gt\"] for rec in truth_df.to_dict(orient=\"records\")}\n",
    "    return text_df, truth_dict, categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a batch with two categories\n",
    "Here we create a batch using the \"autos\" and \"baseball\" categories, and create document vectors of the text for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, truth_dict, categories = create_dataset_for_newsgroup_pair([\"rec.autos\",\"rec.sport.baseball\"])\n",
    "doc_embeddings = sif_embedding_wrapper.sentences2vecs(df[\"text\"], embs, words, weight4ind)\n",
    "df[\"vector\"] = pd.Series(list(doc_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below contains all the classification logic. Given a dataframe of documents, with ID, text, and vector along with a dict specifying the list of documents to use as representatives for each category, the function predicts categories for the remaining documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_classify(docs, category_reps, min_text_length=80):\n",
    "    # Exclude docs deemed too short to classify.\n",
    "    skip_prediction = list(df[df[\"text\"].map(len) < min_text_length].doc_id)\n",
    "    categories = []\n",
    "    for k,v in category_reps.items():\n",
    "        categories.append(k)\n",
    "        skip_prediction.extend(v) # No need to predict manually labeled docs\n",
    "    category_vecs = {}\n",
    "    for c in categories:\n",
    "        vectors = np.asarray(list(docs.loc[docs['doc_id'].isin(category_reps[c])].vector))\n",
    "        category_vecs[c] = np.mean(vectors, axis=0)\n",
    "\n",
    "    predictions = {}\n",
    "    for idx, row in docs.iterrows():\n",
    "        doc_id = row[\"doc_id\"]\n",
    "        if doc_id in skip_prediction:\n",
    "            continue\n",
    "        max_sim = 0\n",
    "        winner = categories[0]\n",
    "        for j in category_vecs:\n",
    "            sim = cosine_similarity(row[\"vector\"].reshape(1, -1), category_vecs[j].reshape(1, -1)).flatten()[0]\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                winner = j\n",
    "        predictions[doc_id] = winner\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the document combination we discovered through our brute-force search through the combinations to provide the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = auto_classify(df, {\"autos\": [\"351\"], \"baseball\": [\"171\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy\n",
    "Here we define a function that simply determines the fraction of correctly predicted documents, and call it with our predictions and the dict containing the ground truth for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9703065134099617"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_accuracy_score(predictions, truth_dict):\n",
    "    scores = []\n",
    "    for k,v in predictions.items():\n",
    "      if v == truth_dict[k]:\n",
    "        scores.append(1)\n",
    "      else:\n",
    "        scores.append(0)\n",
    "    if len(scores) == 0:\n",
    "      return 0.0\n",
    "    return sum(scores) / float(len(scores))\n",
    "\n",
    "get_accuracy_score(preds, truth_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic inference\n",
    "In the above case, we already knew a combination of documents that would produce a high level of accuracy, but in a real scenario we'll use topic inference to try to surface documents that are likely to be good category representatives. We define a function that perferms Latent Dirichlet Allocation on the batch and returns the documents in descending order of \"topiciness\", interleaved according to topic. We then call this function on our \"autos, baseball\" batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_topics(docs, n_topics, min_text_length=80, max_iter=150, batch_size=128, learning_offset=300.):\n",
    "    unclassifiable = list(docs[docs[\"text\"].map(len) < min_text_length].doc_id)\n",
    "    filtered = docs[~docs['doc_id'].isin(unclassifiable)]\n",
    "    ids = [d for d in list(filtered.doc_id)[0:10]]\n",
    "    n_features = 1000\n",
    "    tf_vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        max_df=0.95,\n",
    "        min_df=0.1,\n",
    "        max_features=n_features)\n",
    "    tf = tf_vectorizer.fit_transform(list(filtered.loc[:, 'text']))\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        max_iter=max_iter,\n",
    "        batch_size=batch_size,\n",
    "        learning_method='online',\n",
    "        learning_offset=learning_offset,\n",
    "        random_state=0)\n",
    "    lda.fit(tf)\n",
    "    doc_topics = lda.transform(tf)\n",
    "    topic_leaders = {\"topic_{}\".format(i): [] for i in iter(range(n_topics))}\n",
    "    for idx, probs in enumerate(doc_topics):\n",
    "        score = max(probs)\n",
    "        topic = np.argmax(probs)\n",
    "\n",
    "        doc_id = filtered.loc[filtered.index[idx]].doc_id\n",
    "        topic_leaders[\"topic_{}\".format(topic)].append(\n",
    "            {\"doc_id\": doc_id, \"score\": score})\n",
    "    for i in iter(range(n_topics)):\n",
    "        topic_leaders[\"topic_{}\".format(i)] = sorted(\n",
    "            topic_leaders[\"topic_{}\".format(i)], key=itemgetter('score'), reverse=True)\n",
    "\n",
    "    def roundrobin(*iterables):\n",
    "        \"roundrobin('ABC', 'D', 'EF') --> A D E B F C\"\n",
    "        # Recipe credited to George Sakkis\n",
    "        pending = len(iterables)\n",
    "        nexts = cycle(iter(it).next for it in iterables)\n",
    "        while pending:\n",
    "            try:\n",
    "                for next in nexts:\n",
    "                    yield next()\n",
    "            except StopIteration:\n",
    "                pending -= 1\n",
    "                nexts = cycle(islice(nexts, pending))\n",
    "\n",
    "    return list(roundrobin(*topic_leaders.values()))\n",
    "\n",
    "ordered_docs = infer_topics(df, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get all possible combinations of the top n documents from our LDA ordering, based on their ground truth categories, and calculate prediction accuracy on those combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946360153256705"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_lda_combs(ordered_ids, docs_df, categories, truth_dict, top_n=12):\n",
    "    representatives = {c:[] for c in categories}\n",
    "    for doc_id in ordered_ids[:top_n]:\n",
    "        gt = truth_dict[str(doc_id)]\n",
    "        representatives[gt].append(doc_id)\n",
    "    for c in categories:\n",
    "        if len(representatives[c]) == 0:\n",
    "            print(\"No representatives for %s\" % c)\n",
    "            return None\n",
    "    values = [representatives[c] for c in categories]\n",
    "    doc_combs = list(itertools.product(*values))\n",
    "    return doc_combs\n",
    "\n",
    "def get_lda_accuracies(categories, doc_combs, docs_df, truth_dict):\n",
    "    accuracies = []\n",
    "    for comb in doc_combs:\n",
    "        category_reps = {}\n",
    "        for i,c in enumerate(categories):\n",
    "            category_reps[c] = [str(comb[i])]\n",
    "        preds = auto_classify(docs_df, category_reps)\n",
    "        acc = get_accuracy_score(preds, truth_dict)\n",
    "        accuracies.append(acc)\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "top_lda_combs = get_top_lda_combs([d[\"doc_id\"] for d in ordered_docs], \n",
    "                                  df, categories, truth_dict)\n",
    "lda_accs = get_lda_accuracies(categories, top_lda_combs, df, truth_dict)\n",
    "max(lda_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
